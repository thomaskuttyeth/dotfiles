{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4176e2",
   "metadata": {},
   "source": [
    "## What is sequence-to-sequence learning? \n",
    "Sequence to sequence learning is about training models to covert sequences from one domain to sequences in another domain.When both input sequences and output sequences have the same length, you can implement such models simply with a Keras LSTM or GRU layer (or stack thereof). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb811f",
   "metadata": {},
   "source": [
    "Process\n",
    "----------------------------------------\n",
    "1) Turn the sentences into 3 Numpy arrays, encoder_input_data, decoder_input_data, decoder_target_data:\n",
    "\n",
    "* encoder_input_data is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n",
    "* decoder_input_data is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) containg a one-hot vectorization of the French sentences.\n",
    "* decoder_target_data is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :].\n",
    "\n",
    "2) Train a basic LSTM-based Seq2Seq model to predict decoder_target_data given encoder_input_data and decoder_input_data. Our model uses teacher forcing.\n",
    "\n",
    "3) Decode some sentences to check that the model is working (i.e. turn samples from encoder_input_data into corresponding samples from decoder_target_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba3637fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc88be6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Archive:  fra-eng.zip',\n",
       " 'replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL',\n",
       " '(EOF or read error, treating as \"[N]one\" ...)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
    "!!unzip fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4f8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "latent_dim = 256\n",
    "num_samples = 10000 # no of training samles \n",
    "data_path = 'fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cb6bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_chars = set()\n",
    "target_chars = set()\n",
    "\n",
    "with open(data_path,'r',encoding = 'utf-8') as f:\n",
    "    lines = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "887100f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines[:min(num_samples,len(lines)-1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    \n",
    "    # using tab and \\n as the start and end seq characters. \n",
    "    target_text = '\\t'+target_text+'\\n'\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    for char in input_text:\n",
    "        if char not in input_chars:\n",
    "            input_chars.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_chars:\n",
    "            target_chars.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f962d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the length  parameters to define the room for vectors\n",
    "input_characters = sorted(list(input_chars))\n",
    "target_characters = sorted(list(target_chars))\n",
    "\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters) \n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb681846",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(char,i) for i, char in enumerate(input_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24f950f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts),max_encoder_seq_length,num_encoder_tokens),dtype = \"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length,num_decoder_tokens),dtype = \"float32\"\n",
    ")\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length,num_decoder_tokens),dtype = \"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6476818",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n",
    "    for t,char in enumerate(input_text):\n",
    "        encoder_input_data[i,t,input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i,t+1;, input_token_index[\" \"]] = 1.0\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
